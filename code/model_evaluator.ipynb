{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets learn\n",
    "\n",
    "lets learn how the attention is going to work . curious to know what does the attention block calculate and gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from utils import Normalize\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\",\n",
    "                 temperature=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.bn_att = nn.BatchNorm1d(out_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('input x ',x.shape)\n",
    "       \n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        print('input norm_att ',norm_att.shape)\n",
    "        #print('this is the value ',norm_att)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        print('cla ',cla.shape)\n",
    "        \n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        print('sum shape ',x.shape)\n",
    "        print('sum shape ',x)\n",
    "        \n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class HpaSub(nn.Module):\n",
    "    def __init__(self, classes, features):\n",
    "        super(HpaSub, self).__init__()\n",
    "        self.species = nn.Sequential(\n",
    "            nn.Linear(features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(512, classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, GAP):\n",
    "        #print('GAP ',GAP.shape)\n",
    "        GAP = F.avg_pool2d(GAP, GAP.size()[2:]).squeeze()\n",
    "        #rint('GAP ',GAP.shape)\n",
    "        spe = self.species(GAP)\n",
    "        return spe\n",
    "\n",
    "class HpaModel(nn.Module):\n",
    "    def __init__(self, classes, device, base_model_name, pretrained, features):\n",
    "        super(HpaModel, self).__init__()\n",
    "        self.base_model_name = base_model_name\n",
    "        mean_list = [0.083170892049318, 0.08627143702844145, 0.05734662013795027, 0.06582942296076659,0.0]\n",
    "        std_list = [0.13561066140407024, 0.13301454127989584, 0.09142918497144226, 0.15651865713966945,1.]\n",
    "        self.transform=transforms.Compose([Normalize(mean= mean_list,\n",
    "                              std= std_list,\n",
    "                              device = device)])\n",
    "\n",
    "        if 'efficientnet' in self.base_model_name:\n",
    "            self.model = EfficientNet.from_pretrained(self.base_model_name)#torch.hub.load('lukemelas/EfficientNet-PyTorch', self.base_model_name, pretrained=pretrained)\n",
    "            #print(self.model)\n",
    "        else:\n",
    "            base_model = torch.hub.load('zhanghang1989/ResNeSt', self.base_model_name, pretrained=pretrained) \n",
    "            #print('the list ',list(base_model.children()))\n",
    "            layers = list(base_model.children())[:-2]\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        self.init_layer = nn.Conv2d(in_channels=5, out_channels=3, kernel_size=1, stride=1,bias= True)\n",
    "        self.fc1 = nn.Linear(features, features, bias=True)\n",
    "        self.att_block = AttBlock(features, classes, activation=\"linear\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, cells, C, H, W = x.size()\n",
    "        c_in = self.transform(x.view(batch_size * cells, C, H, W))\n",
    "        #print('input c_in ',c_in.shape)\n",
    "        c_in = F.relu(self.init_layer(c_in))\n",
    "        #print('init layer c_in ',c_in.shape)\n",
    "        if 'efficientnet' in self.base_model_name:\n",
    "            spe = self.model.extract_features(c_in)\n",
    "        else:\n",
    "            spe = self.model(c_in)\n",
    "        spe = F.avg_pool2d(spe, spe.size()[2:]).squeeze()\n",
    "        #print('enc shape ',spe.shape)\n",
    "        spe = F.relu(self.fc1(F.dropout(spe.contiguous().view(batch_size, cells, -1), p=0.5, training=self.training))).permute(0,2,1)\n",
    "        #print('spe shape ',spe.shape)\n",
    "        final_output, norm_att, cell_pred = self.att_block(F.dropout(spe, p=0.5, training=self.training))\n",
    "\n",
    "        return {'final_output':final_output, 'cell_pred':cell_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = HpaModel(19, torch.device('cpu'), 'efficientnet-b0', True, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.from_numpy(np.zeros((1,4,5,224,224))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 5, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x  torch.Size([1, 1280, 4])\n",
      "input norm_att  torch.Size([1, 19, 4])\n",
      "cla  torch.Size([1, 19, 4])\n",
      "sum shape  torch.Size([1, 19])\n",
      "sum shape  tensor([[ 0.3346, -0.2422, -0.0350, -0.0470,  0.1430, -0.0996, -0.0047, -0.0552,\n",
      "          0.4018, -0.2664,  0.1328, -0.0169, -0.0669,  0.1333,  0.1455,  0.0601,\n",
      "         -0.0338, -0.0407, -0.0215]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "output = model(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
