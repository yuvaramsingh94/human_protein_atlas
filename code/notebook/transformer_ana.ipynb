{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_master\n"
     ]
    }
   ],
   "source": [
    "base_model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "layers = list(base_model.children())[:-2]\n",
    "model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros((1,3,256,256))\n",
    "output = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "output_flatten = output.flatten(2).transpose(1, 2)\n",
    "print(output_flatten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cls token and position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the general form . we will break it down\n",
    "\n",
    "\n",
    "class ViTEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct the CLS token, position and patch embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,num_patches, hidden_size, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_size))\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        embeddings = pixel_values.flatten(2).transpose(1, 2)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_emb = ViTEmbeddings(num_patches = 64, hidden_size = 512, hidden_dropout_prob = 0.0)\n",
    "embedding = vit_emb(model(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = nn.Linear(512, 512)\n",
    "key = nn.Linear(512, 512)\n",
    "value = nn.Linear(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_value = query(embedding)\n",
    "key_value = key(embedding)\n",
    "value_value = value(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_atten_layer = nn.MultiheadAttention(embed_dim = 256, num_heads = 8, dropout=0.0, bias=True, \n",
    "                                               add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output, attn_output_weights = multi_head_atten_layer(query_value, key_value, value_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-56926c14f307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattn_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach()."
     ]
    }
   ],
   "source": [
    "#attn_output[:,0,:].requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    layer_head_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets try our adoptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import Normalize\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\",\n",
    "                 temperature=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.bn_att = nn.BatchNorm1d(out_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)# tanh(self.att(x))\n",
    "        #norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        #print('norm_att ',norm_att.shape)\n",
    "        #print('normal ',norm_att)\n",
    "        #print('normal sum ',torch.sum(norm_att, dim =-1))\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        #print('cla ',cla.shape)\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        #print('sum ',x.shape)\n",
    "        #x = torch.clamp(x, min=0.0, max = 1.0)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "class ViTEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct the CLS token, position and patch embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_patches, hidden_size, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_size))\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        embeddings = pixel_values.flatten(2).transpose(1, 2)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)    \n",
    "\n",
    "class HpaSub(nn.Module):\n",
    "    def __init__(self, classes, features):\n",
    "        super(HpaSub, self).__init__()\n",
    "        self.species = nn.Sequential(\n",
    "            nn.Linear(features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(512, classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, GAP):\n",
    "        #print('GAP ',GAP.shape)\n",
    "        GAP = F.avg_pool2d(GAP, GAP.size()[2:]).squeeze()\n",
    "        #rint('GAP ',GAP.shape)\n",
    "        spe = self.species(GAP)\n",
    "        return spe\n",
    "\n",
    "class attention_encoding(nn.Module):\n",
    "    def __init__(self, embedding_dims, num_patches, hidden_size, hidden_dropout_prob, attention_heads, is_first = True):\n",
    "        super(attention_encoding, self).__init__()\n",
    "        self.vit_emb = ViTEmbeddings(num_patches = num_patches, \n",
    "                        hidden_size = hidden_size, hidden_dropout_prob = hidden_dropout_prob)\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.emb_norm = nn.LayerNorm(self.embedding_dims)\n",
    "        #self.emb_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.query = nn.Linear(self.embedding_dims, self.embedding_dims)\n",
    "        self.key = nn.Linear(self.embedding_dims, self.embedding_dims)\n",
    "        self.value = nn.Linear(self.embedding_dims, self.embedding_dims)\n",
    "\n",
    "        self.multi_head_atten_layer = nn.MultiheadAttention(embed_dim = self.embedding_dims, num_heads = attention_heads,\n",
    "                                dropout=hidden_dropout_prob, bias=True, add_bias_kv=False, \n",
    "                                add_zero_attn=False, kdim=None, vdim=None)\n",
    "        self.attention_norm = nn.LayerNorm(self.embedding_dims)\n",
    "        self.mlp = nn.Linear(self.embedding_dims, self.embedding_dims)\n",
    "        #self.mlp_norm = nn.LayerNorm(self.embedding_dims)\n",
    "        self.is_first = is_first\n",
    "\n",
    "    def forward(self, s0):\n",
    "        if self.is_first:## only embed for the first layer \n",
    "            s0 = self.vit_emb(s0)\n",
    "        x = self.emb_norm(s0)\n",
    "        query_ = self.query(x)\n",
    "        key_ = self.key(x)\n",
    "        value_ = self.value(x)\n",
    "        attn_output, _ = self.multi_head_atten_layer(query_, key_, value_)\n",
    "        s1 = attn_output + s0\n",
    "        x = self.attention_norm(s1)\n",
    "        x = F.relu(self.mlp(x))\n",
    "        s2 = x + s1\n",
    "\n",
    "        return s2\n",
    "\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "class HpaModel_1(nn.Module):\n",
    "    def __init__(self, classes, device, base_model_name, pretrained, features, spe_drop = 0.5, att_drop = 0.5):\n",
    "        super(HpaModel_1, self).__init__()\n",
    "        self.base_model_name = base_model_name\n",
    "        self.classes = classes\n",
    "        self.features = features\n",
    "        self.spe_drop = spe_drop\n",
    "        self.att_drop = att_drop\n",
    "        mean_list = [0.083170892049318, 0.08627143702844145, 0.05734662013795027, 0.06582942296076659,0.0]\n",
    "        std_list = [0.13561066140407024, 0.13301454127989584, 0.09142918497144226, 0.15651865713966945,1.]\n",
    "        self.transform=transforms.Compose([Normalize(mean= mean_list,\n",
    "                              std= std_list,\n",
    "                              device = device)])\n",
    "\n",
    "        if 'efficientnet' in self.base_model_name:\n",
    "            self.model = EfficientNet.from_pretrained(self.base_model_name)#torch.hub.load('lukemelas/EfficientNet-PyTorch', self.base_model_name, pretrained=pretrained)\n",
    "            #print(self.model)\n",
    "        elif 'resnet' in self.base_model_name:\n",
    "            base_model = torch.hub.load('pytorch/vision', base_model_name, pretrained=pretrained)\n",
    "            layers = list(base_model.children())[:-2]\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        else:\n",
    "            base_model = torch.hub.load('zhanghang1989/ResNeSt', self.base_model_name, pretrained=pretrained) \n",
    "            #print('the list ',list(base_model.children()))\n",
    "            layers = list(base_model.children())[:-2]\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        self.init_layer = nn.Conv2d(in_channels=5, out_channels=3, kernel_size=1, stride=1,bias= False)\n",
    "        self.batch_norm_init = nn.BatchNorm2d(3)\n",
    "\n",
    "        self.attention_encoding = attention_encoding(embedding_dims = self.features, num_patches = 64, hidden_size = self.features, \n",
    "                                    hidden_dropout_prob = 0.0, attention_heads = 8, is_first = True)\n",
    "        self.att_mlp_norm = nn.LayerNorm(self.features)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.features, self.features, bias=True)\n",
    "        self.att_block = AttBlock(self.features, classes, activation=\"linear\")\n",
    "\n",
    "    def init_attention_layer(self,):\n",
    "        print('hi')\n",
    "        self.fc1 = nn.Linear(self.features, self.features, bias=True)\n",
    "        self.att_block = AttBlock(self.features, self.classes, activation=\"linear\")\n",
    "\n",
    "    def trainable_parameters(self):\n",
    "        return (list(nn.ModuleList([self.init_layer, self.batch_norm_init, self.model, self.attention_encoding,\n",
    "                    self.att_mlp_norm, self.fc1, self.att_block]).parameters()), \n",
    "                list(nn.ModuleList([self.fc1, self.att_block]).parameters()))\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        batch_size, cells, C, H, W = x.size()\n",
    "        c_in = self.transform(x.view(batch_size * cells, C, H, W))\n",
    "        #print('input c_in ',c_in.shape)\n",
    "        c_in = F.relu(self.batch_norm_init(self.init_layer(c_in)))\n",
    "        #print('init layer c_in ',c_in.shape)\n",
    "        if 'efficientnet' in self.base_model_name:\n",
    "            spe = self.model.extract_features(c_in)\n",
    "        else:\n",
    "            spe = self.model(c_in)\n",
    "        spe = F.avg_pool2d(spe, spe.size()[2:]).squeeze()\n",
    "\n",
    "        return spe.contiguous().view(batch_size, cells, -1)\n",
    "    \n",
    "    def attention_section(self,spe):\n",
    "        spe = F.relu(self.fc1(F.dropout(spe, p=self.spe_drop, training=self.training))).permute(0,2,1)\n",
    "        #print('spe shape ',spe.shape)\n",
    "        final_output, norm_att, cell_pred = self.att_block(F.dropout(spe, p=self.att_drop, training=self.training))\n",
    "        cell_pred = torch.sigmoid(cell_pred)\n",
    "        return {'final_output':final_output, 'cell_pred':cell_pred}\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, cells, C, H, W = x.size()\n",
    "        c_in = self.transform(x.view(batch_size * cells, C, H, W))\n",
    "        #print('input c_in ',c_in.shape)\n",
    "        c_in = F.relu(self.batch_norm_init(self.init_layer(c_in)))\n",
    "        #print('init layer c_in ',c_in.shape)\n",
    "        if 'efficientnet' in self.base_model_name:\n",
    "            spe = self.model.extract_features(c_in)\n",
    "        else:\n",
    "            spe = self.model(c_in)\n",
    "        # attention pooling\n",
    "        #print('spe shape ',spe.shape)\n",
    "        spe = self.attention_encoding(spe)\n",
    "        spe = self.att_mlp_norm(spe)\n",
    "\n",
    "        #print('att shape ',spe.shape)\n",
    "        spe = spe[:,0,:].squeeze()\n",
    "        #print('cls feature shape  ',spe.shape)\n",
    "        \n",
    "        spe = F.relu(self.fc1(F.dropout(spe.contiguous().view(batch_size, cells, -1), p=self.spe_drop, training=self.training))).permute(0,2,1)\n",
    "        #print('spe shape ',spe.shape)\n",
    "        final_output, norm_att, cell_pred = self.att_block(F.dropout(spe, p=self.att_drop, training=self.training))\n",
    "        cell_pred = torch.sigmoid(cell_pred)\n",
    "        return {'final_output':final_output, 'cell_pred':cell_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_master\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = HpaModel_1(classes = 19, device = device, base_model_name = 'resnet18', features = 512, pretrained = True, spe_drop = 0.2, att_drop = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros((2,16,5,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spe shape  torch.Size([32, 512, 8, 8])\n",
      "att shape  torch.Size([32, 65, 512])\n",
      "cls feature shape   torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "v = model(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
